# Unittest framework for Python

After getting some experiences from work, slowly I start to realize that the unit test can be very useful in some circumstances. Without sufficient and reliable unit test, some of the codes will never give users confidence of using them in practice.

Previously, I create unit tests manually, and call all the functions. The output is not very well structured, and whenever we hit an assertion, the whole test will stop. This is not too bad, but a framework might be helpful.

In Python, `unittest` is the builtin unit test framework to create and run unit tests. Check the files `test_mathfunc.py` and `test_suite.py` in this directory and see how this framework works.

- TestCase: methods of a subclass of `unittest.TestCase`, start with `test_xxx`, and will be considered as a single unittest. `setUp` and `tearDown` are two useful methods, to create common environment and clean the environment after the test finishes.
- TestSuite: collections of tests and run each of them and generate a report.
- TestLoader: I feel like the most useful bit of this is to read all the tests in a module and create a test suite.

One useful thing is `unittest.skip`, which can be used as decorators. This indicates that "we don't want to run this particular test at the moment". A unsolved test method can be skipped by using this decorator.

# Some improvements you can do for the test

Looks like unit tests in Python 2.7 has some features missing, which are added in Python 3. So in order to make the work simple, I want to add these features so I can use them in this year.

- MockObject: Sometimes we want to replace part of the object by mock object. A mock object should
    - has exactly the same methods as the original object
    - maybe same attributes, but I would rather have the same method instead of attributes
    - sometimes the output of the methods are defaulted/determined.
- PureFunctionTestCase: I would love to test whether a function is pure, by
    - make sure it is not using any global variables (which should be done by not setting any global variables)
    - make sure the input object is not modified after running the function
    - generic method to compare to objects
    - make sure the output is completely determined by the inputs (stateless). This can be achieved by blur tests.
- BlurTest: input a lot of random inputs, and the output is always as expected.
    - generators of random inputs by giving the format of the inputs
    - run the test a lot of times and compare the results to expected outputs
- RegressionTest
    - compare old results with new results
    - should have an easy setup for both old and new programs
    - compare using something instead of meld
- EdgeCaseAutoGenerator: sometimes we may want to exhaustly test the code by using some very unusual inputs. I wish we can generate these inputs automatically.
    - EdgeNumberGenerator: for int and float, we should be able to generate
        - negative numbers
        - 0
        - very very small floats
        - a very very large positive number
        - a very very "small" negative number
    - EdgeListGenerator: for a list, should use list with length 0.
    - None
    - Bool: exhaustly generate True and False
    - Merge the above generators into an object and generate weird attributes?


# Immediate application

In the MachineLearning framework, one of the concern is the components and model may be out-of-date. This means if we have changed the code at some time, and we are still using old results, we should run into trouble.

I think a unit test running every a few days can be helpful. The most important part is the cached data, generated by components, and skipped since then.

One way to solve the problem is to record the input/output filenames, and the name of the component in a test_component_config file or directory, and use a unit test to automatically generate the output data using input data and component, and compare with cached output data saved in cached files.

How can we do this?
- Create a test_component_config.txt somewhere
    - Create a class/function in LazyDataframe to automatically write the test_component_config.txt.
    - Use a bash script or python program to recursively go through all the data directory, and output the config file each time we run the test.
- For each component, write a test function and check all the input/output data pair. If it fails, output the msg.
